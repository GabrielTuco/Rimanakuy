{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4ErAKf7j1Qs"
   },
   "source": [
    "#LM-prior-nmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uqtSuezV1UQS",
    "outputId": "f8b3f099-71d8-4a0c-e346-1cfbbbc1f509"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.14\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jf7ANmmYDx19",
    "outputId": "fe255ec6-79e9-48c3-d163-177b074bd9bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Rimanakuy\n"
     ]
    }
   ],
   "source": [
    "%cd Rimanakuy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "634_43j_D0K1"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/content/Rimanakuy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5cQPddqrDrWp",
    "outputId": "031e89cb-eae6-4e17-aaa2-00d86bfcd228"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting typing~=3.6.6 (from -r /content/Rimanakuy/requirements.txt (line 1))\n",
      "  Downloading typing-3.6.6-py3-none-any.whl (25 kB)\n",
      "Collecting torchtext~=0.4.0 (from -r /content/Rimanakuy/requirements.txt (line 2))\n",
      "  Downloading torchtext-0.4.0-py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m466.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r /content/Rimanakuy/requirements.txt (line 3)) (2.2.1+cu121)\n",
      "Requirement already satisfied: numpy~=1.20 in /usr/local/lib/python3.10/dist-packages (from -r /content/Rimanakuy/requirements.txt (line 4)) (1.25.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from -r /content/Rimanakuy/requirements.txt (line 5)) (6.0.1)\n",
      "Collecting sacrebleu~=1.4.2 (from -r /content/Rimanakuy/requirements.txt (line 6))\n",
      "  Downloading sacrebleu-1.4.14-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.8/64.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r /content/Rimanakuy/requirements.txt (line 7)) (3.7.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r /content/Rimanakuy/requirements.txt (line 8)) (4.66.4)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r /content/Rimanakuy/requirements.txt (line 9)) (4.40.2)\n",
      "Collecting pandas~=1.5.3 (from -r /content/Rimanakuy/requirements.txt (line 10))\n",
      "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting seaborn~=0.9.0 (from -r /content/Rimanakuy/requirements.txt (line 11))\n",
      "  Downloading seaborn-0.9.1-py2.py3-none-any.whl (216 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.4/216.4 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from -r /content/Rimanakuy/requirements.txt (line 12)) (0.9.0)\n",
      "Collecting oyaml~=0.9 (from -r /content/Rimanakuy/requirements.txt (line 13))\n",
      "  Downloading oyaml-0.9-py2.py3-none-any.whl (3.0 kB)\n",
      "Collecting umap~=0.1.1 (from -r /content/Rimanakuy/requirements.txt (line 14))\n",
      "  Downloading umap-0.1.1.tar.gz (3.2 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r /content/Rimanakuy/requirements.txt (line 15)) (1.2.2)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from -r /content/Rimanakuy/requirements.txt (line 16)) (0.1.99)\n",
      "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from -r /content/Rimanakuy/requirements.txt (line 17)) (4.3.2)\n",
      "Collecting sacremoses~=0.0.35 (from -r /content/Rimanakuy/requirements.txt (line 18))\n",
      "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r /content/Rimanakuy/requirements.txt (line 19)) (1.11.4)\n",
      "Requirement already satisfied: glob2~=0.6 in /usr/local/lib/python3.10/dist-packages (from -r /content/Rimanakuy/requirements.txt (line 20)) (0.7)\n",
      "Collecting visdom~=0.1.8.9 (from -r /content/Rimanakuy/requirements.txt (line 21))\n",
      "  Downloading visdom-0.1.8.9.tar.gz (676 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m676.6/676.6 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from -r /content/Rimanakuy/requirements.txt (line 22)) (0.20.3)\n",
      "Collecting dill (from -r /content/Rimanakuy/requirements.txt (line 23))\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from -r /content/Rimanakuy/requirements.txt (line 24)) (2.15.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext~=0.4.0->-r /content/Rimanakuy/requirements.txt (line 2)) (2.31.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext~=0.4.0->-r /content/Rimanakuy/requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/Rimanakuy/requirements.txt (line 3)) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/Rimanakuy/requirements.txt (line 3)) (4.11.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/Rimanakuy/requirements.txt (line 3)) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/Rimanakuy/requirements.txt (line 3)) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/Rimanakuy/requirements.txt (line 3)) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/Rimanakuy/requirements.txt (line 3)) (2023.6.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->-r /content/Rimanakuy/requirements.txt (line 3))\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->-r /content/Rimanakuy/requirements.txt (line 3))\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->-r /content/Rimanakuy/requirements.txt (line 3))\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->-r /content/Rimanakuy/requirements.txt (line 3))\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->-r /content/Rimanakuy/requirements.txt (line 3))\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->-r /content/Rimanakuy/requirements.txt (line 3))\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->-r /content/Rimanakuy/requirements.txt (line 3))\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->-r /content/Rimanakuy/requirements.txt (line 3))\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->-r /content/Rimanakuy/requirements.txt (line 3))\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from torch->-r /content/Rimanakuy/requirements.txt (line 3))\n",
      "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->-r /content/Rimanakuy/requirements.txt (line 3))\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/Rimanakuy/requirements.txt (line 3)) (2.2.0)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->-r /content/Rimanakuy/requirements.txt (line 3))\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "Collecting portalocker (from sacrebleu~=1.4.2->-r /content/Rimanakuy/requirements.txt (line 6))\n",
      "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/Rimanakuy/requirements.txt (line 7)) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/Rimanakuy/requirements.txt (line 7)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/Rimanakuy/requirements.txt (line 7)) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/Rimanakuy/requirements.txt (line 7)) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/Rimanakuy/requirements.txt (line 7)) (24.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/Rimanakuy/requirements.txt (line 7)) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/Rimanakuy/requirements.txt (line 7)) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/Rimanakuy/requirements.txt (line 7)) (2.8.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers->-r /content/Rimanakuy/requirements.txt (line 9)) (0.20.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r /content/Rimanakuy/requirements.txt (line 9)) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->-r /content/Rimanakuy/requirements.txt (line 9)) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r /content/Rimanakuy/requirements.txt (line 9)) (0.4.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas~=1.5.3->-r /content/Rimanakuy/requirements.txt (line 10)) (2023.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r /content/Rimanakuy/requirements.txt (line 15)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r /content/Rimanakuy/requirements.txt (line 15)) (3.5.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->-r /content/Rimanakuy/requirements.txt (line 17)) (6.4.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses~=0.0.35->-r /content/Rimanakuy/requirements.txt (line 18)) (8.1.7)\n",
      "Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from visdom~=0.1.8.9->-r /content/Rimanakuy/requirements.txt (line 21)) (6.3.3)\n",
      "Requirement already satisfied: pyzmq in /usr/local/lib/python3.10/dist-packages (from visdom~=0.1.8.9->-r /content/Rimanakuy/requirements.txt (line 21)) (24.0.1)\n",
      "Collecting jsonpatch (from visdom~=0.1.8.9->-r /content/Rimanakuy/requirements.txt (line 21))\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting torchfile (from visdom~=0.1.8.9->-r /content/Rimanakuy/requirements.txt (line 21))\n",
      "  Downloading torchfile-0.1.0.tar.gz (5.2 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from visdom~=0.1.8.9->-r /content/Rimanakuy/requirements.txt (line 21)) (1.8.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/Rimanakuy/requirements.txt (line 24)) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/Rimanakuy/requirements.txt (line 24)) (1.63.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/Rimanakuy/requirements.txt (line 24)) (2.27.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/Rimanakuy/requirements.txt (line 24)) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/Rimanakuy/requirements.txt (line 24)) (3.6)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/Rimanakuy/requirements.txt (line 24)) (3.20.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/Rimanakuy/requirements.txt (line 24)) (67.7.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/Rimanakuy/requirements.txt (line 24)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/Rimanakuy/requirements.txt (line 24)) (3.0.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r /content/Rimanakuy/requirements.txt (line 24)) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r /content/Rimanakuy/requirements.txt (line 24)) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r /content/Rimanakuy/requirements.txt (line 24)) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->-r /content/Rimanakuy/requirements.txt (line 24)) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext~=0.4.0->-r /content/Rimanakuy/requirements.txt (line 2)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext~=0.4.0->-r /content/Rimanakuy/requirements.txt (line 2)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext~=0.4.0->-r /content/Rimanakuy/requirements.txt (line 2)) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext~=0.4.0->-r /content/Rimanakuy/requirements.txt (line 2)) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->-r /content/Rimanakuy/requirements.txt (line 24)) (2.1.5)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch->visdom~=0.1.8.9->-r /content/Rimanakuy/requirements.txt (line 21))\n",
      "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r /content/Rimanakuy/requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r /content/Rimanakuy/requirements.txt (line 24)) (0.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->-r /content/Rimanakuy/requirements.txt (line 24)) (3.2.2)\n",
      "Building wheels for collected packages: umap, sacremoses, visdom, torchfile\n",
      "  Building wheel for umap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for umap: filename=umap-0.1.1-py3-none-any.whl size=3543 sha256=14b2150c45d370a19553d4f2c99d7070426496aeb941e49abe67730f940eeb0b\n",
      "  Stored in directory: /root/.cache/pip/wheels/15/f1/28/53dcf7a309118ed35d810a5f9cb995217800f3f269ab5771cb\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895240 sha256=abcf3fff9c11d01a3353da5d62905000670890a02dd1da1e5473a54e03386cc8\n",
      "  Stored in directory: /root/.cache/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\n",
      "  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for visdom: filename=visdom-0.1.8.9-py3-none-any.whl size=655231 sha256=18259027999fd14aab875023cad9ce997a3f38ff8e087adeb8d777454eeadb0f\n",
      "  Stored in directory: /root/.cache/pip/wheels/4f/d8/38/63755b5e437983f6a2dca8ec8e9b959fc8e4964753ff73922d\n",
      "  Building wheel for torchfile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for torchfile: filename=torchfile-0.1.0-py3-none-any.whl size=5693 sha256=8c209ff8be64a1d929054b5f38a0ca5154b2949951112ec19c7bbbc811b25197\n",
      "  Stored in directory: /root/.cache/pip/wheels/c7/e9/87/1c51daf8e468d5c14931f8ac3344880f903ba96b063675cac2\n",
      "Successfully built umap sacremoses visdom torchfile\n",
      "Installing collected packages: umap, typing, torchfile, sacremoses, portalocker, oyaml, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, jsonpointer, dill, sacrebleu, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jsonpatch, visdom, seaborn, nvidia-cusolver-cu12, torchtext\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.0.3\n",
      "    Uninstalling pandas-2.0.3:\n",
      "      Successfully uninstalled pandas-2.0.3\n",
      "  Attempting uninstall: seaborn\n",
      "    Found existing installation: seaborn 0.13.1\n",
      "    Uninstalling seaborn-0.13.1:\n",
      "      Successfully uninstalled seaborn-0.13.1\n",
      "  Attempting uninstall: torchtext\n",
      "    Found existing installation: torchtext 0.17.1\n",
      "    Uninstalling torchtext-0.17.1:\n",
      "      Successfully uninstalled torchtext-0.17.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf-cu12 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas==2.0.3, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed dill-0.3.8 jsonpatch-1.33 jsonpointer-2.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 oyaml-0.9 pandas-1.5.3 portalocker-2.8.2 sacrebleu-1.4.14 sacremoses-0.0.53 seaborn-0.9.1 torchfile-0.1.0 torchtext-0.4.0 typing-3.6.6 umap-0.1.1 visdom-0.1.8.9\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "7c85de7e1d19404f8dd12c14e607c986",
       "pip_warning": {
        "packages": [
         "typing"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install -r /content/Rimanakuy/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "purwOmGGNBFx",
    "outputId": "3c14ef81-f61f-44b2-ebae-0e0b62753e88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-20 19:53:28.246661: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-20 19:53:28.246717: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-20 19:53:28.247982: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-20 19:53:29.374380: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "config:configs/transformer/trans.ques_base.yaml\n",
      "name:final.trans.ques_base\n",
      "desc:None\n",
      "tag:None\n",
      "visdom:False\n",
      "pin_memory:False\n",
      "resume_cp:None\n",
      "resume_state_id:None\n",
      "device:cuda\n",
      "cores:2\n",
      "src_dirs:['/content/Rimanakuy/models', '/content/Rimanakuy/modules', '/content/Rimanakuy/helpers']\n",
      "\n",
      "{'batch_tokens': 10000,\n",
      " 'config': 'configs/transformer/trans.ques_base.yaml',\n",
      " 'cores': 2,\n",
      " 'data': {'lowercase': True,\n",
      "          'prior_path': None,\n",
      "          'seq_len': 1000,\n",
      "          'sos': True,\n",
      "          'src': {'lang': 'de',\n",
      "                  'subword_path': '/content/Rimanakuy/datasets/source',\n",
      "                  'test_path': '/content/Rimanakuy/datasets/quechuaPRPE.quy.test',\n",
      "                  'train_path': '/content/Rimanakuy/datasets/quechuaPRPE.quy.train',\n",
      "                  'val_path': '/content/Rimanakuy/datasets/quechuaPRPE.quy.dev'},\n",
      "          'streaming': False,\n",
      "          'trg': {'lang': 'en',\n",
      "                  'subword_path': '/content/Rimanakuy/datasets/target',\n",
      "                  'test_path': '/content/Rimanakuy/datasets/spanishPRPE.es.test',\n",
      "                  'train_path': '/content/Rimanakuy/datasets/spanishPRPE.es.train',\n",
      "                  'val_path': '/content/Rimanakuy/datasets/spanishPRPE.es.dev'},\n",
      "          'vocab_path': None,\n",
      "          'vocab_size': None},\n",
      " 'desc': None,\n",
      " 'device': 'cuda',\n",
      " 'epochs': 2,\n",
      " 'init': {'embed_init': 'xavier_uniform',\n",
      "          'linear_init': 'xavier_uniform',\n",
      "          'lstm_hh_init': 'orthogonal',\n",
      "          'lstm_ih_init': 'xavier_uniform'},\n",
      " 'logging': {'checkpoint_interval': 50,\n",
      "             'emb_inspect_interval': 200,\n",
      "             'eval_interval': 1000,\n",
      "             'full_eval_interval': 4000000,\n",
      "             'log_interval': 50,\n",
      "             'module_grad_interval': 200,\n",
      "             'samples_interval': 200},\n",
      " 'losses': {'mt': {'perplexity': True,\n",
      "                   'smoothing': 0.0,\n",
      "                   'tag': 'translation',\n",
      "                   'weight': 1}},\n",
      " 'model': {'decoding': {'fusion': None},\n",
      "           'dropout': 0.3,\n",
      "           'emb_renorm': False,\n",
      "           'emb_size': 512,\n",
      "           'emb_trainable': True,\n",
      "           'nhead': 8,\n",
      "           'nhid': 1024,\n",
      "           'nlayers': 6,\n",
      "           'tie_projections': True,\n",
      "           'type': 'transformer'},\n",
      " 'name': 'final.trans.ques_base',\n",
      " 'optim': {'clip': 1,\n",
      "           'early_stop': 10,\n",
      "           'eta_min': 1e-05,\n",
      "           'factor': 1,\n",
      "           'gamma': 0.5,\n",
      "           'interval': 'batch',\n",
      "           'k': 10,\n",
      "           'lr': 0.0003,\n",
      "           'milestones': [5, 15],\n",
      "           'min_lr': 1e-05,\n",
      "           'optimizer': 'adam',\n",
      "           'patience': 3,\n",
      "           'scheduler': 'noam',\n",
      "           'step_size': 1,\n",
      "           'warmup': 8000,\n",
      "           'weight_decay': 0.0},\n",
      " 'pin_memory': False,\n",
      " 'resume_cp': None,\n",
      " 'resume_state_id': None,\n",
      " 'src_dirs': ['/content/Rimanakuy/models',\n",
      "              '/content/Rimanakuy/modules',\n",
      "              '/content/Rimanakuy/helpers'],\n",
      " 'tag': None,\n",
      " 'transfer': {'emb': None, 'lm': None, 'tm_path': None},\n",
      " 'visdom': False}\n",
      "{'epochs': 2, 'batch_tokens': 10000, 'logging': {'log_interval': 50, 'eval_interval': 1000, 'full_eval_interval': 4000000, 'checkpoint_interval': 50, 'samples_interval': 200, 'emb_inspect_interval': 200, 'module_grad_interval': 200}, 'init': {'linear_init': 'xavier_uniform', 'lstm_hh_init': 'orthogonal', 'lstm_ih_init': 'xavier_uniform', 'embed_init': 'xavier_uniform'}, 'optim': {'optimizer': 'adam', 'lr': 0.0003, 'k': 10, 'weight_decay': 0.0, 'clip': 1, 'scheduler': 'noam', 'interval': 'batch', 'factor': 1, 'warmup': 8000, 'step_size': 1, 'patience': 3, 'eta_min': 1e-05, 'min_lr': 1e-05, 'gamma': 0.5, 'milestones': [5, 15], 'early_stop': 10}, 'data': {'src': {'train_path': '/content/Rimanakuy/datasets/quechuaPRPE.quy.train', 'val_path': '/content/Rimanakuy/datasets/quechuaPRPE.quy.dev', 'test_path': '/content/Rimanakuy/datasets/quechuaPRPE.quy.test', 'subword_path': '/content/Rimanakuy/datasets/source', 'lang': 'de'}, 'trg': {'train_path': '/content/Rimanakuy/datasets/spanishPRPE.es.train', 'val_path': '/content/Rimanakuy/datasets/spanishPRPE.es.dev', 'test_path': '/content/Rimanakuy/datasets/spanishPRPE.es.test', 'subword_path': '/content/Rimanakuy/datasets/target', 'lang': 'en'}, 'seq_len': 1000, 'lowercase': True, 'sos': True, 'streaming': False, 'vocab_path': None, 'vocab_size': None, 'prior_path': None}, 'losses': {'mt': {'weight': 1, 'tag': 'translation', 'perplexity': True, 'smoothing': 0.0}}, 'transfer': {'tm_path': None, 'lm': None, 'emb': None}, 'model': {'emb_renorm': False, 'type': 'transformer', 'decoding': {'fusion': None}, 'emb_trainable': True, 'emb_size': 512, 'nhid': 1024, 'nhead': 8, 'nlayers': 6, 'dropout': 0.3, 'tie_projections': True}, 'name': 'final.trans.ques_base', 'desc': None, 'config': 'configs/transformer/trans.ques_base.yaml', 'tag': None, 'visdom': False, 'pin_memory': False, 'resume_cp': None, 'resume_state_id': None, 'device': 'cuda', 'cores': 2, 'src_dirs': ['/content/Rimanakuy/models', '/content/Rimanakuy/modules', '/content/Rimanakuy/helpers']}\n",
      "Building training dataset...\n",
      "Preprocessing...\n",
      "The cache file is out-of-date\n",
      "Caching lines to /content/Rimanakuy/datasets/quechuaPRPE.quy.train.cache\n",
      "100% 10529283/10529491 [00:00<00:00, 25746359.58it/s]\n",
      "Writing data to cache...\n",
      "Preprocessing...\n",
      "The cache file is out-of-date\n",
      "Caching lines to /content/Rimanakuy/datasets/spanishPRPE.es.train.cache\n",
      "100% 11115155/11115319 [00:00<00:00, 23158803.60it/s]\n",
      "Writing data to cache...\n",
      "file                     examples    vocab    tokens (unique)  tokens (total)      max length\n",
      "---------------------  ----------  -------  -----------------  ----------------  ------------\n",
      "quechuaPRPE.quy.train      113840    38218              38215  1.8M                      1000\n",
      "spanishPRPE.es.train       113840    21214              21211  2.5M                      1000\n",
      "\n",
      "Building validation dataset...\n",
      "Preprocessing...\n",
      "The cache file is out-of-date\n",
      "Caching lines to /content/Rimanakuy/datasets/quechuaPRPE.quy.dev.cache\n",
      "100% 187789/187953 [00:00<00:00, 20165497.17it/s]\n",
      "Writing data to cache...\n",
      "Preprocessing...\n",
      "The cache file is out-of-date\n",
      "Caching lines to /content/Rimanakuy/datasets/spanishPRPE.es.dev.cache\n",
      "100% 198363/198505 [00:00<00:00, 20717515.98it/s]\n",
      "Writing data to cache...\n",
      "file                   examples    vocab    tokens (unique)  tokens (total)      max length\n",
      "-------------------  ----------  -------  -----------------  ----------------  ------------\n",
      "quechuaPRPE.quy.dev        2000    38218              38215  32.2K                     1000\n",
      "spanishPRPE.es.dev         2000    21214              21211  44.7K                     1000\n",
      "\n",
      "Seq2SeqTransformer(\n",
      "  (embed_src): Embed(\n",
      "    (embedding): Embedding(38218, 512, padding_idx=0)\n",
      "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (embed_tgt): Embed(\n",
      "    (embedding): Embedding(21214, 512, padding_idx=0)\n",
      "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (encoder): TransformerEncoder(num_layers=6, num_heads=8)\n",
      "  (decoder): TransformerDecoder(num_layers=6, num_heads=8)\n",
      ")\n",
      "Total Params: 62.0M\n",
      "Total Trainable Params: 62.0M\n",
      "Starting with state id:None...\n",
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:1, Batch:50/433(12%) Time: 0m 29s (-3m 48s)\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:1, Batch:100/433(23%) Time: 0m 58s (-3m 15s)\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:1, Batch:150/433(35%) Time: 1m 28s (-2m 46s)\n",
      "[==============--------------------------]\n",
      "\u001b[KLoss:     8.13\tPerplexity:  3390.73\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:1, Batch:200/433(46%) Time: 1m 58s (-2m 17s)\n",
      "[==================----------------------]\n",
      "\u001b[KLoss:     7.55\tPerplexity:  1905.96\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:1, Batch:250/433(58%) Time: 2m 29s (-1m 49s)\n",
      "[=======================-----------------]\n",
      "\u001b[KLoss:     6.96\tPerplexity:  1051.71\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:1, Batch:300/433(69%) Time: 3m 1s (-1m 20s)\n",
      "[============================------------]\n",
      "\u001b[KLoss:     6.23\tPerplexity:   506.04\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:1, Batch:350/433(81%) Time: 3m 31s (-0m 50s)\n",
      "[================================--------]\n",
      "\u001b[KLoss:     5.46\tPerplexity:   236.24\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:1, Batch:400/433(92%) Time: 4m 3s (-0m 20s)\n",
      "[=====================================---]\n",
      "\u001b[KLoss:     4.68\tPerplexity:   108.07\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\n",
      "Epoch 1: Average training statistics\n",
      "----------------------------------------\n",
      "TRAIN\tLoss:     6.96\tPerplexity:  1048.86\n",
      "\n",
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "\n",
      "Epoch 1: Average validation statistics\n",
      "----------------------------------------\n",
      "VALID\tLoss:     3.85\tPerplexity:    46.92\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:2, Batch:17/433(4%) Time: 0m 10s (-4m 26s)\n",
      "[==--------------------------------------]\n",
      "\u001b[KLoss:     3.80\tPerplexity:    44.74\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:2, Batch:67/433(15%) Time: 0m 42s (-3m 50s)\n",
      "[======----------------------------------]\n",
      "\u001b[KLoss:     3.62\tPerplexity:    37.20\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:2, Batch:117/433(27%) Time: 1m 13s (-3m 18s)\n",
      "[===========-----------------------------]\n",
      "\u001b[KLoss:     3.53\tPerplexity:    34.05\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:2, Batch:167/433(39%) Time: 1m 44s (-2m 46s)\n",
      "[===============-------------------------]\n",
      "\u001b[KLoss:     3.47\tPerplexity:    32.09\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:2, Batch:217/433(50%) Time: 2m 15s (-2m 14s)\n",
      "[====================--------------------]\n",
      "\u001b[KLoss:     3.38\tPerplexity:    29.43\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:2, Batch:267/433(62%) Time: 2m 45s (-1m 43s)\n",
      "[=========================---------------]\n",
      "\u001b[KLoss:     3.32\tPerplexity:    27.54\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:2, Batch:317/433(73%) Time: 3m 17s (-1m 12s)\n",
      "[=============================-----------]\n",
      "\u001b[KLoss:     3.26\tPerplexity:    26.07\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:2, Batch:367/433(85%) Time: 3m 48s (-0m 41s)\n",
      "[==================================------]\n",
      "\u001b[KLoss:     3.24\tPerplexity:    25.63\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:2, Batch:417/433(96%) Time: 4m 19s (-0m 9s)\n",
      "[=======================================-]\n",
      "\u001b[KLoss:     3.18\tPerplexity:    24.13\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\n",
      "Epoch 2: Average training statistics\n",
      "----------------------------------------\n",
      "TRAIN\tLoss:     3.38\tPerplexity:    29.51\n",
      "\n",
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "\n",
      "Epoch 2: Average validation statistics\n",
      "----------------------------------------\n",
      "VALID\tLoss:     3.14\tPerplexity:    23.18\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<models.nmt_prior_trainer.NmtPriorTrainer object at 0x78b0d4c3bd00>\n"
     ]
    }
   ],
   "source": [
    "!python models/nmt_prior.py --config configs/transformer/trans.ques_base.yaml --device cuda --name final.trans.ques_base --cores 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hlJiwVlAfkrs",
    "outputId": "edb9cfe4-b16a-4d5a-e6a7-0faa9002a7f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-08 03:04:51.298814: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-08 03:04:51.298865: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-08 03:04:51.300189: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-08 03:04:52.488006: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "config:configs/transformer/trans.ques_base.yaml\n",
      "name:final.trans.ques_base\n",
      "desc:None\n",
      "tag:None\n",
      "visdom:False\n",
      "pin_memory:False\n",
      "resume_cp:None\n",
      "resume_state_id:None\n",
      "device:cuda\n",
      "cores:2\n",
      "src_dirs:['/content/Rimanakuy/models', '/content/Rimanakuy/modules', '/content/Rimanakuy/helpers']\n",
      "\n",
      "{'batch_tokens': 10000,\n",
      " 'config': 'configs/transformer/trans.ques_base.yaml',\n",
      " 'cores': 2,\n",
      " 'data': {'lowercase': True,\n",
      "          'prior_path': None,\n",
      "          'seq_len': 1000,\n",
      "          'sos': True,\n",
      "          'src': {'lang': 'de',\n",
      "                  'subword_path': '/content/Rimanakuy/datasets/source',\n",
      "                  'test_path': '/content/Rimanakuy/datasets/quechuaPRPE.quy.test',\n",
      "                  'train_path': '/content/Rimanakuy/datasets/quechuaPRPE.quy.train',\n",
      "                  'val_path': '/content/Rimanakuy/datasets/quechuaPRPE.quy.dev'},\n",
      "          'streaming': False,\n",
      "          'trg': {'lang': 'en',\n",
      "                  'subword_path': '/content/Rimanakuy/datasets/target',\n",
      "                  'test_path': '/content/Rimanakuy/datasets/spanishPRPE.es.test',\n",
      "                  'train_path': '/content/Rimanakuy/datasets/spanishPRPE.es.train',\n",
      "                  'val_path': '/content/Rimanakuy/datasets/spanishPRPE.es.dev'},\n",
      "          'vocab_path': None,\n",
      "          'vocab_size': None},\n",
      " 'desc': None,\n",
      " 'device': 'cuda',\n",
      " 'epochs': 5,\n",
      " 'init': {'embed_init': 'xavier_uniform',\n",
      "          'linear_init': 'xavier_uniform',\n",
      "          'lstm_hh_init': 'orthogonal',\n",
      "          'lstm_ih_init': 'xavier_uniform'},\n",
      " 'logging': {'checkpoint_interval': 50,\n",
      "             'emb_inspect_interval': 200,\n",
      "             'eval_interval': 1000,\n",
      "             'full_eval_interval': 4000000,\n",
      "             'log_interval': 50,\n",
      "             'module_grad_interval': 200,\n",
      "             'samples_interval': 200},\n",
      " 'losses': {'mt': {'perplexity': True,\n",
      "                   'smoothing': 0.0,\n",
      "                   'tag': 'translation',\n",
      "                   'weight': 1}},\n",
      " 'model': {'decoding': {'fusion': None},\n",
      "           'dropout': 0.3,\n",
      "           'emb_renorm': False,\n",
      "           'emb_size': 512,\n",
      "           'emb_trainable': True,\n",
      "           'nhead': 8,\n",
      "           'nhid': 1024,\n",
      "           'nlayers': 6,\n",
      "           'tie_projections': True,\n",
      "           'type': 'transformer'},\n",
      " 'name': 'final.trans.ques_base',\n",
      " 'optim': {'clip': 1,\n",
      "           'early_stop': 10,\n",
      "           'eta_min': 1e-05,\n",
      "           'factor': 1,\n",
      "           'gamma': 0.5,\n",
      "           'interval': 'batch',\n",
      "           'k': 10,\n",
      "           'lr': 0.0003,\n",
      "           'milestones': [5, 15],\n",
      "           'min_lr': 1e-05,\n",
      "           'optimizer': 'adam',\n",
      "           'patience': 3,\n",
      "           'scheduler': 'noam',\n",
      "           'step_size': 1,\n",
      "           'warmup': 8000,\n",
      "           'weight_decay': 0.0},\n",
      " 'pin_memory': False,\n",
      " 'resume_cp': None,\n",
      " 'resume_state_id': None,\n",
      " 'src_dirs': ['/content/Rimanakuy/models',\n",
      "              '/content/Rimanakuy/modules',\n",
      "              '/content/Rimanakuy/helpers'],\n",
      " 'tag': None,\n",
      " 'transfer': {'emb': None, 'lm': None, 'tm_path': None},\n",
      " 'visdom': False}\n",
      "Building training dataset...\n",
      "Loading data from cache... done!\n",
      "Preprocessing...\n",
      "Loading data from cache... done!\n",
      "Preprocessing...\n",
      "file                     examples    vocab    tokens (unique)  tokens (total)      max length\n",
      "---------------------  ----------  -------  -----------------  ----------------  ------------\n",
      "quechuaPRPE.quy.train      113840    38218              38215  1.8M                      1000\n",
      "spanishPRPE.es.train       113840    21214              21211  2.5M                      1000\n",
      "\n",
      "Building validation dataset...\n",
      "Loading data from cache... done!\n",
      "Preprocessing...\n",
      "Loading data from cache... done!\n",
      "Preprocessing...\n",
      "file                   examples    vocab    tokens (unique)  tokens (total)      max length\n",
      "-------------------  ----------  -------  -----------------  ----------------  ------------\n",
      "quechuaPRPE.quy.dev        2000    38218              38215  32.2K                     1000\n",
      "spanishPRPE.es.dev         2000    21214              21211  44.7K                     1000\n",
      "\n",
      "Seq2SeqTransformer(\n",
      "  (embed_src): Embed(\n",
      "    (embedding): Embedding(38218, 512, padding_idx=0)\n",
      "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (embed_tgt): Embed(\n",
      "    (embedding): Embedding(21214, 512, padding_idx=0)\n",
      "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (encoder): TransformerEncoder(num_layers=6, num_heads=8)\n",
      "  (decoder): TransformerDecoder(num_layers=6, num_heads=8)\n",
      ")\n",
      "Total Params: 62.0M\n",
      "Total Trainable Params: 62.0M\n",
      "Starting with state id:None...\n",
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:1, Batch:50/432(12%) Time: 0m 33s (-4m 13s)\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:1, Batch:100/432(23%) Time: 1m 5s (-3m 38s)\n",
      "[=========-------------------------------]Error processing batch: 139. Trying again...\n",
      "CUDA out of memory. Tried to allocate 88.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 5.06 MiB is free. Process 67144 has 14.74 GiB memory in use. Of the allocated memory 14.21 GiB is allocated by PyTorch, and 404.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:1, Batch:150/432(35%) Time: 1m 37s (-3m 3s)\n",
      "[==============--------------------------]\n",
      "\u001b[KLoss:     8.10\tPerplexity:  3290.62\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:1, Batch:200/432(46%) Time: 2m 10s (-2m 31s)\n",
      "[===================---------------------]\n",
      "\u001b[KLoss:     7.53\tPerplexity:  1854.75\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:1, Batch:250/432(58%) Time: 2m 42s (-1m 58s)\n",
      "[=======================-----------------]\n",
      "\u001b[KLoss:     6.91\tPerplexity:   997.94\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:1, Batch:300/432(69%) Time: 3m 14s (-1m 25s)\n",
      "[============================------------]\n",
      "\u001b[KLoss:     6.17\tPerplexity:   478.18\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:1, Batch:350/432(81%) Time: 3m 47s (-0m 53s)\n",
      "[================================--------]\n",
      "\u001b[KLoss:     5.38\tPerplexity:   217.96\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:1, Batch:400/432(93%) Time: 4m 19s (-0m 20s)\n",
      "[=====================================---]\n",
      "\u001b[KLoss:     4.63\tPerplexity:   102.10\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\n",
      "Epoch 1: Average training statistics\n",
      "----------------------------------------\n",
      "TRAIN\tLoss:     6.93\tPerplexity:  1017.46\n",
      "\n",
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "\n",
      "Epoch 1: Average validation statistics\n",
      "----------------------------------------\n",
      "VALID\tLoss:     3.83\tPerplexity:    46.05\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:2, Batch:18/432(4%) Time: 0m 11s (-4m 33s)\n",
      "[==--------------------------------------]\n",
      "\u001b[KLoss:     3.81\tPerplexity:    45.35\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:2, Batch:68/432(16%) Time: 0m 44s (-3m 58s)\n",
      "[======----------------------------------]\n",
      "\u001b[KLoss:     3.60\tPerplexity:    36.76\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:2, Batch:118/432(27%) Time: 1m 16s (-3m 23s)\n",
      "[===========-----------------------------]\n",
      "\u001b[KLoss:     3.52\tPerplexity:    33.64\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:2, Batch:168/432(39%) Time: 1m 48s (-2m 50s)\n",
      "[================------------------------]\n",
      "\u001b[KLoss:     3.46\tPerplexity:    31.79\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:2, Batch:218/432(50%) Time: 2m 20s (-2m 17s)\n",
      "[====================--------------------]\n",
      "\u001b[KLoss:     3.45\tPerplexity:    31.61\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:2, Batch:268/432(62%) Time: 2m 51s (-1m 44s)\n",
      "[=========================---------------]\n",
      "\u001b[KLoss:     3.37\tPerplexity:    28.95\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:2, Batch:318/432(74%) Time: 3m 24s (-1m 13s)\n",
      "[=============================-----------]\n",
      "\u001b[KLoss:     3.29\tPerplexity:    26.80\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:2, Batch:368/432(85%) Time: 3m 56s (-0m 41s)\n",
      "[==================================------]\n",
      "\u001b[KLoss:     3.24\tPerplexity:    25.54\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:2, Batch:418/432(97%) Time: 4m 29s (-0m 9s)\n",
      "[=======================================-]\n",
      "\u001b[KLoss:     3.18\tPerplexity:    24.00\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\n",
      "Epoch 2: Average training statistics\n",
      "----------------------------------------\n",
      "TRAIN\tLoss:     3.40\tPerplexity:    29.88\n",
      "\n",
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "\n",
      "Epoch 2: Average validation statistics\n",
      "----------------------------------------\n",
      "VALID\tLoss:     3.15\tPerplexity:    23.27\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:3, Batch:35/432(8%) Time: 0m 22s (-4m 12s)\n",
      "[===-------------------------------------]\n",
      "\u001b[KLoss:     3.12\tPerplexity:    22.75\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:3, Batch:85/432(20%) Time: 0m 54s (-3m 42s)\n",
      "[========--------------------------------]\n",
      "\u001b[KLoss:     3.11\tPerplexity:    22.42\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:3, Batch:135/432(31%) Time: 1m 26s (-3m 10s)\n",
      "[============----------------------------]\n",
      "\u001b[KLoss:     3.08\tPerplexity:    21.75\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "Translating (beam=1)...:   0% 0/8 [00:00<?, ?it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Translating (beam=1)...: 100% 8/8 [00:45<00:00,  5.69s/it]\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:3, Batch:185/432(43%) Time: 2m 44s (-3m 39s)\n",
      "[=================-----------------------]\n",
      "\u001b[KLoss:     3.05\tPerplexity:    21.18\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:3, Batch:235/432(54%) Time: 3m 17s (-2m 45s)\n",
      "[======================------------------]\n",
      "\u001b[KLoss:     3.05\tPerplexity:    21.03\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:3, Batch:285/432(66%) Time: 3m 49s (-1m 58s)\n",
      "[==========================--------------]\n",
      "\u001b[KLoss:     3.02\tPerplexity:    20.47\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:3, Batch:335/432(78%) Time: 4m 21s (-1m 15s)\n",
      "[===============================---------]\n",
      "\u001b[KLoss:     3.00\tPerplexity:    20.09\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:3, Batch:385/432(89%) Time: 4m 53s (-0m 35s)\n",
      "[====================================----]\n",
      "\u001b[KLoss:     3.01\tPerplexity:    20.38\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\n",
      "Epoch 3: Average training statistics\n",
      "----------------------------------------\n",
      "TRAIN\tLoss:     3.05\tPerplexity:    21.01\n",
      "\n",
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "\n",
      "Epoch 3: Average validation statistics\n",
      "----------------------------------------\n",
      "VALID\tLoss:     2.95\tPerplexity:    19.14\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:4, Batch:2/432(0%) Time: 0m 2s (-7m 46s)\n",
      "[----------------------------------------]\n",
      "\u001b[KLoss:     2.96\tPerplexity:    19.39\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:4, Batch:52/432(12%) Time: 0m 34s (-4m 12s)\n",
      "[=====-----------------------------------]\n",
      "\u001b[KLoss:     2.97\tPerplexity:    19.59\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:4, Batch:102/432(24%) Time: 1m 5s (-3m 33s)\n",
      "[=========-------------------------------]\n",
      "\u001b[KLoss:     2.97\tPerplexity:    19.44\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:4, Batch:152/432(35%) Time: 1m 38s (-3m 1s)\n",
      "[==============--------------------------]\n",
      "\u001b[KLoss:     2.92\tPerplexity:    18.56\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:4, Batch:202/432(47%) Time: 2m 10s (-2m 28s)\n",
      "[===================---------------------]\n",
      "\u001b[KLoss:     2.92\tPerplexity:    18.47\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:4, Batch:252/432(58%) Time: 2m 43s (-1m 56s)\n",
      "[=======================-----------------]\n",
      "\u001b[KLoss:     2.91\tPerplexity:    18.35\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:4, Batch:302/432(70%) Time: 3m 15s (-1m 24s)\n",
      "[============================------------]\n",
      "\u001b[KLoss:     2.85\tPerplexity:    17.35\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:4, Batch:352/432(81%) Time: 3m 47s (-0m 51s)\n",
      "[=================================-------]\n",
      "\u001b[KLoss:     2.85\tPerplexity:    17.25\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:4, Batch:402/432(93%) Time: 4m 19s (-0m 19s)\n",
      "[=====================================---]\n",
      "\u001b[KLoss:     2.84\tPerplexity:    17.17\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\n",
      "Epoch 4: Average training statistics\n",
      "----------------------------------------\n",
      "TRAIN\tLoss:     2.90\tPerplexity:    18.13\n",
      "\n",
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "\n",
      "Epoch 4: Average validation statistics\n",
      "----------------------------------------\n",
      "VALID\tLoss:     2.79\tPerplexity:    16.28\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:5, Batch:19/432(4%) Time: 0m 12s (-4m 33s)\n",
      "[==--------------------------------------]\n",
      "\u001b[KLoss:     2.83\tPerplexity:    16.91\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:5, Batch:69/432(16%) Time: 0m 44s (-3m 54s)\n",
      "[======----------------------------------]\n",
      "\u001b[KLoss:     2.80\tPerplexity:    16.50\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:5, Batch:119/432(28%) Time: 1m 16s (-3m 21s)\n",
      "[===========-----------------------------]\n",
      "\u001b[KLoss:     2.80\tPerplexity:    16.40\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:5, Batch:169/432(39%) Time: 1m 48s (-2m 48s)\n",
      "[================------------------------]\n",
      "\u001b[KLoss:     2.78\tPerplexity:    16.07\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:5, Batch:219/432(51%) Time: 2m 20s (-2m 17s)\n",
      "[====================--------------------]\n",
      "\u001b[KLoss:     2.73\tPerplexity:    15.37\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:5, Batch:269/432(62%) Time: 2m 53s (-1m 45s)\n",
      "[=========================---------------]\n",
      "\u001b[KLoss:     2.72\tPerplexity:    15.13\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "Translating (beam=1)...:   0% 0/8 [00:00<?, ?it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Translating (beam=1)...: 100% 8/8 [00:45<00:00,  5.71s/it]\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:5, Batch:319/432(74%) Time: 4m 11s (-1m 28s)\n",
      "[==============================----------]\n",
      "\u001b[KLoss:     2.72\tPerplexity:    15.25\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:5, Batch:369/432(85%) Time: 4m 43s (-0m 48s)\n",
      "[==================================------]\n",
      "\u001b[KLoss:     2.70\tPerplexity:    14.85\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\u001b[K--------------------\n",
      "final.trans.ques_base\n",
      "--------------------\n",
      "Epoch:5, Batch:419/432(97%) Time: 5m 14s (-0m 9s)\n",
      "[=======================================-]\n",
      "\u001b[KLoss:     2.72\tPerplexity:    15.11\n",
      "\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\u001b[F\n",
      "\n",
      "Epoch 5: Average training statistics\n",
      "----------------------------------------\n",
      "TRAIN\tLoss:     2.75\tPerplexity:    15.62\n",
      "\n",
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "\n",
      "Epoch 5: Average validation statistics\n",
      "----------------------------------------\n",
      "VALID\tLoss:     2.66\tPerplexity:    14.32\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loading data from cache... done!\n",
      "Preprocessing...\n",
      "Loading data from cache... done!\n",
      "Preprocessing...\n",
      "file                   examples    vocab    tokens (unique)  tokens (total)      max length\n",
      "-------------------  ----------  -------  -----------------  ----------------  ------------\n",
      "quechuaPRPE.quy.dev        2000    38218              38215  32.2K                     1000\n",
      "spanishPRPE.es.dev         2000    21214              21211  44.7K                     1000\n",
      "\n",
      "Translating...: 100% 8/8 [00:02<00:00,  3.23it/s]\n",
      "Preprocessing...\n",
      "Caching lines to /content/Rimanakuy/datasets/quechuaPRPE.quy.test.cache\n",
      "100% 184710/184903 [00:00<00:00, 18676740.96it/s]\n",
      "Writing data to cache...\n",
      "Preprocessing...\n",
      "Caching lines to /content/Rimanakuy/datasets/spanishPRPE.es.test.cache\n",
      "100% 193984/194172 [00:00<00:00, 21650555.27it/s]\n",
      "Writing data to cache...\n",
      "file                    examples    vocab    tokens (unique)  tokens (total)      max length\n",
      "--------------------  ----------  -------  -----------------  ----------------  ------------\n",
      "quechuaPRPE.quy.test        2000    38218              38215  31.9K                     1000\n",
      "spanishPRPE.es.test         2000    21214              21211  43.9K                     1000\n",
      "\n",
      "Translating...: 100% 8/8 [00:02<00:00,  3.52it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/Rimanakuy/models/nmt_prior.py\", line 145, in <module>\n",
      "    eval_best(trained_model)\n",
      "  File \"/content/Rimanakuy/./models/nmt_prior_helpers.py\", line 121, in eval_best\n",
      "    eval_nmt_checkpoint(trainer.best_checkpoint,\n",
      "  File \"/content/Rimanakuy/./models/translate.py\", line 110, in eval_nmt_checkpoint\n",
      "    _base, _file = os.path.split(checkpoint)\n",
      "  File \"/usr/lib/python3.10/posixpath.py\", line 103, in split\n",
      "    p = os.fspath(p)\n",
      "TypeError: expected str, bytes or os.PathLike object, not NoneType\n"
     ]
    }
   ],
   "source": [
    "!python models/nmt_prior.py --config configs/transformer/trans.ques_base.yaml --device cuda --name final.trans.ques_base --cores 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"E:\\Tesis\\Code\\Rimanakuy\\models\\nmt_prior.py\", line 9, in <module>\n",
      "    from helpers.training import load_checkpoint\n",
      "  File \"E:\\Tesis\\Code\\Rimanakuy\\.\\helpers\\training.py\", line 7, in <module>\n",
      "    from modules.data.vocab import Vocab\n",
      "  File \"E:\\Tesis\\Code\\Rimanakuy\\.\\modules\\data\\vocab.py\", line 8, in <module>\n",
      "    import seaborn as sns\n",
      "  File \"C:\\Users\\Gabriel\\anaconda3\\envs\\Rimanaky\\lib\\site-packages\\seaborn\\__init__.py\", line 14, in <module>\n",
      "    from .matrix import *\n",
      "  File \"C:\\Users\\Gabriel\\anaconda3\\envs\\Rimanaky\\lib\\site-packages\\seaborn\\matrix.py\", line 14, in <module>\n",
      "    from . import cm\n",
      "  File \"C:\\Users\\Gabriel\\anaconda3\\envs\\Rimanaky\\lib\\site-packages\\seaborn\\cm.py\", line 1055, in <module>\n",
      "    mpl_cm.register_cmap(_name, _cmap)\n",
      "AttributeError: module 'matplotlib.cm' has no attribute 'register_cmap'\n"
     ]
    }
   ],
   "source": [
    "!python models/nmt_prior.py --config configs/transformer/trans.ques_base.yaml --device cpu --name final.trans.ques_base "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9zFQsMm1Uo3o"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "sys.path.insert(0, '..')\n",
    "from torch import nn\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "from helpers.opts import exp_options\n",
    "from helpers.training import load_checkpoint\n",
    "from helpers.transfer import freeze_module\n",
    "from models.nmt_prior_callbacks import EvalCallback, SamplesCallback, \\\n",
    "    AttentionCallback\n",
    "from models.nmt_prior_helpers import nmt_dataloaders, \\\n",
    "    eval_best, backtranslate\n",
    "from models.translate import prior_model_from_checkpoint\n",
    "from models.nmt_prior_trainer import NmtPriorTrainer\n",
    "from modules.callbacks import LossCallback, GradientCallback, \\\n",
    "    ModuleGradientCallback, FunctionCallback\n",
    "from modules.data.vocab import Vocab\n",
    "from modules.initializations import model_init\n",
    "from modules.models import Seq2SeqTransformer, Seq2SeqRNN\n",
    "from sys_config import MODEL_CNF_DIR\n",
    "\n",
    "\n",
    "def run(config):\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Load pretrained models\n",
    "    # -------------------------------------------------------------------------\n",
    "    vocab_src = None\n",
    "    vocab_trg = None\n",
    "\n",
    "    # Load pretrained LM, which will be used for LM-Fusion or as LM-prior\n",
    "    if config[\"data\"][\"prior_path\"] is not None:\n",
    "        if \"gpt2\" in config[\"data\"][\"prior_path\"]:\n",
    "            _gpt_model = os.path.split(config[\"data\"][\"prior_path\"])[1]\n",
    "            tokenizer = GPT2Tokenizer.from_pretrained(_gpt_model)\n",
    "            vocab_trg = Vocab()\n",
    "            vocab_trg.from_gpt2(tokenizer)\n",
    "            _checkp_prior = GPT2LMHeadModel.from_pretrained(_gpt_model)\n",
    "            config[\"model\"][\"dec_padding_idx\"] = None\n",
    "        else:\n",
    "            _checkp_prior = load_checkpoint(config[\"data\"][\"prior_path\"])\n",
    "            vocab_trg = _checkp_prior[\"vocab\"]\n",
    "\n",
    "            if _checkp_prior[\"config\"][\"data\"][\"subword_path\"] is not None:\n",
    "                sub_path = _checkp_prior[\"config\"][\"data\"][\"subword_path\"]\n",
    "                config[\"data\"][\"trg\"][\"subword_path\"] = sub_path\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Data Loading and Preprocessing\n",
    "    # -------------------------------------------------------------------------\n",
    "    train_loader, val_loader = nmt_dataloaders(config, vocab_src, vocab_trg)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Initialize Model and Priors\n",
    "    # -------------------------------------------------------------------------\n",
    "    model_type = config[\"model\"].get(\"type\", \"rnn\")\n",
    "    src_ntokens = len(val_loader.dataset.src.vocab)\n",
    "    trg_ntokens = len(val_loader.dataset.trg.vocab)\n",
    "\n",
    "    # Initialize Model\n",
    "    if model_type == \"rnn\":\n",
    "        model = Seq2SeqRNN(src_ntokens, trg_ntokens, **config[\"model\"])\n",
    "    elif model_type == \"transformer\":\n",
    "        model = Seq2SeqTransformer(src_ntokens, trg_ntokens, **config[\"model\"])\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    model_init(model, **config.get(\"init\", {}))\n",
    "\n",
    "    # Initialize prior LM\n",
    "    _has_lm_prior = \"prior\" in config[\"losses\"]\n",
    "    _has_lm_fusion = config[\"model\"][\"decoding\"].get(\"fusion\") is not None\n",
    "    if _has_lm_prior or _has_lm_fusion:\n",
    "        if \"gpt2\" in config[\"data\"][\"prior_path\"]:\n",
    "            prior = _checkp_prior\n",
    "            prior.to(config[\"device\"])\n",
    "            freeze_module(prior)\n",
    "            for name, module in prior.named_modules():\n",
    "                if isinstance(module, nn.Dropout):\n",
    "                    module.p = 0\n",
    "        else:\n",
    "            prior = prior_model_from_checkpoint(_checkp_prior)\n",
    "            prior.to(config[\"device\"])\n",
    "            freeze_module(prior)\n",
    "    else:\n",
    "        prior = None\n",
    "\n",
    "    model.tie_weights()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Training Pipeline\n",
    "    # -------------------------------------------------------------------------\n",
    "    callbacks = [\n",
    "        LossCallback(config[\"logging\"][\"log_interval\"]),\n",
    "        GradientCallback(config[\"logging\"][\"log_interval\"]),\n",
    "        ModuleGradientCallback([\"encoder\"], config[\"logging\"][\"log_interval\"]),\n",
    "        SamplesCallback(config[\"logging\"][\"log_interval\"]),\n",
    "        EvalCallback(config[\"logging\"][\"eval_interval\"], keep_best=True,\n",
    "                     early_stop=config[\"optim\"][\"early_stop\"])\n",
    "    ]\n",
    "    if model_type == \"rnn\":\n",
    "        callbacks.append(AttentionCallback(config[\"logging\"][\"eval_interval\"]))\n",
    "\n",
    "    eval_interval = config[\"logging\"][\"eval_interval\"]\n",
    "    full_eval_interval = config[\"logging\"].get(\"full_eval_interval\",\n",
    "                                               15 * eval_interval)\n",
    "    callbacks.append(FunctionCallback(eval_best, full_eval_interval))\n",
    "\n",
    "    trainer = NmtPriorTrainer(model, train_loader, val_loader, config,\n",
    "                              config[\"device\"],\n",
    "                              prior=prior, callbacks=callbacks,\n",
    "                              src_dirs=config[\"src_dirs\"],\n",
    "                              resume_state_id=config[\"resume_state_id\"])\n",
    "\n",
    "    if trainer.exp.has_finished():\n",
    "        return trainer\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Training Loop\n",
    "    # -------------------------------------------------------------------------\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        train_loss = trainer.train_epoch()\n",
    "        val_loss = trainer.eval_epoch()\n",
    "        print(\"\\n\" * 3)\n",
    "\n",
    "        if trainer.early_stop:\n",
    "            print(\"Stopping early ...\")\n",
    "            break\n",
    "\n",
    "    trainer.exp.finalize()\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g0ATrfbJMVMn"
   },
   "outputs": [],
   "source": [
    "!cp -R /content/Rimanakuy/experiments/final.trans.ques_base /content/drive/MyDrive/experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QsPMAHKjT9a4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "vgojXTC5jwxh"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
